import shutil
from typing import Union
from fastapi import FastAPI
import fastapi
from fastapi.templating import Jinja2Templates
from fastapi.responses import ORJSONResponse
import numpy as np
from IPython.display import Audio
import librosa
import librosa.display
from sklearn.preprocessing import StandardScaler
import sys
from scipy.io import wavfile
import math
import shutil
from pathlib import Path
from tempfile import NamedTemporaryFile
from typing import Callable

from fastapi import UploadFile
from fastapi.templating import Jinja2Templates
import fastapi
from fastapi_chameleon import template
from starlette.requests import Request
from fastapi_chameleon import template
from fastapi import FastAPI, File, UploadFile, Request
from starlette.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi import FastAPI, File, Form, UploadFile
import os

import speech_recognition as sr
import os
from pydub import AudioSegment
from pydub.silence import split_on_silence
import pandas as pd
import nltk
# nltk.download('punkt')
app = FastAPI()
from fastapi.middleware.cors import CORSMiddleware

origins = [
    "http://localhost",
    "http://localhost:8000",
    "http://localhost:8080",
    "http://localhost:3000",
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def read_root():
    return {"Hello": "World"}


@app.post("/uploade")
async def create_upload_file(request: Request, file: UploadFile = File(...)):
    with open("1.wav", "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    wavFile = '1.wav'
    AUDIO_FILE = wavFile

    recognizer = sr.Recognizer()
    directoryPath = "./Vt/"

    # split audio file into chunks and convert audio to text
    def get_audio_paths(path):

        audio = AudioSegment.from_wav(path)
        # split audio sound where silence is 700 miliseconds or more and get chunks
        chunks = split_on_silence(audio, min_silence_len=1000, silence_thresh=audio.dBFS - 9, keep_silence=800)
        folder_name = directoryPath + "/chunks"
        # store chunks in the following directory (create if not exists)
        if not os.path.isdir(folder_name):
            os.mkdir(folder_name)
        final_text = ""
        # for each chunk
        for i, chunk in enumerate(chunks, start=1):

            filename = os.path.join(folder_name, f"chunk{i}.wav")
            chunk.export(filename, format="wav")
            # recognize the audio
            with sr.AudioFile(filename) as source:
                recognizer.adjust_for_ambient_noise(source, duration=0.4)
                audio_listened = recognizer.record(source)
                # convert audio to text
                try:
                    text = recognizer.recognize_google(audio_listened, language='ar')
                    print(text)
                except sr.UnknownValueError as e:
                    print("Error:", str(e))
                else:
                    final_text += text.lower() + "."
        return final_text

    # paths = ['chunks/chunk1.wav','chunks/chunk2.wav','chunks/chunk3.wav']
    paths = []

    for i, file in enumerate(os.listdir(directoryPath)):
        if file.endswith(".wav"):
            # print(os.path.basename(file))
            print(os.path.basename(file) + str(i))
            paths.append(os.path.basename(file))

    list1 = []
    for i, path in enumerate(paths):
        print(i)
        print(path)
        list1.append(get_audio_paths('1.wav'))

    dataframe = pd.DataFrame(list1, columns=['Text'])
    dataframe.to_csv("sample.csv")

    # # data in excel sheet with Column "Text"
    statusFile = dataframe
    df = dataframe
    df = df.dropna()
    # speech_obj={}
    speech_obj = []

    count = 1
    for index, row in df.iterrows():
        speech = row.Text.split(".")
        for sentence in speech:
            if count % 2 == 0:
                count = 2
            else:
                count = 1
            list1.append({   sentence})

    for item in list1:
        print(item)
        # print("test")
    list1 = list1[1:]
    txt = pd.read_csv('sample.csv')
    ttx = txt.iloc[0]['Text']
    tokenized_txt = nltk.word_tokenize(ttx)
    # print(tokenized_txt)
    dic = {'الدفاع المدني': 'حريق', 'المرور': 'عطل', 'جنائي': "مخدرات", 'جنائي': 'اختطاف', 'غير محدد': ' '}

    val = dict((v, k) for k, v in dic.items())

    def show_key(x):
        try:
            return val[x]
        except:
            pass
    topic= 'غير محدد'
    for i in tokenized_txt:
        if show_key(i) != None:
            topic=show_key(i)
            print(show_key(i))
    return {'list': list1,'topic':topic}

@app.post("/uploade2")
async def create_upload_file(request: Request, file: UploadFile = File(...)):
    with open("./Vt/1.wav", "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    wavFile = './Vt/1.wav'
    AUDIO_FILE = wavFile
    y, sr = librosa.load(AUDIO_FILE)
    Audio(data=y, rate=sr)
    totDuration = librosa.get_duration(filename=AUDIO_FILE)
    # Create chunks/windows for feeding into the model
    division_per_second = 1
    chunk_time = 1.0 / division_per_second
    chunk_size = sr // division_per_second

    remainder_chunks = y.shape[0] % chunk_size
    num_of_chunks = 1
    if (remainder_chunks > 0):
        num_of_chunks = y[:-remainder_chunks].shape[0] / chunk_size
        Y = np.split(y[:-remainder_chunks], num_of_chunks)
    else:
        num_of_chunks = y.shape[0] / chunk_size
        Y = np.split(y, num_of_chunks)

    # Extract feature: Mel-frequency Cepstral Coefficients
    feature_mfcc = np.array([librosa.feature.mfcc(y=chunk, sr=sr, n_mfcc=40) for chunk in Y])
    feature_mfcc.shape
    feature_mfcc_mean = np.mean(feature_mfcc, axis=2)
    # Extract feature: Spectral flatness
    feature_spectral_flatness = np.array([librosa.feature.spectral_flatness(y=y) for chunk in Y])
    feature_spectral_flatness.shape
    feature_spectral_flatness_mean = np.mean(feature_spectral_flatness, axis=2)
    # Extract feature: Spectral flux
    feature_specflux = np.array([librosa.onset.onset_strength(y=y, sr=sr) for chunk in Y])
    feature_specflux.shape
    feature_specflux_mean = np.mean(feature_specflux, axis=1).reshape(-1, 1)
    # Extract feature: Pitch
    feature_pitches = np.array([librosa.piptrack(y=y, sr=sr)[0] for chunk in Y])
    feature_pitches.shape
    feature_pitches_mean = np.mean(feature_pitches, axis=2)
    # Create final feature space for feeding into the model
    X = np.hstack((
        feature_mfcc_mean,
        feature_spectral_flatness_mean,
        feature_specflux_mean,
        feature_pitches_mean
    ))
    X.shape
    # Normalize the input
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    X.shape
    from sklearn.mixture import GaussianMixture
    nclusters = 6
    gmm = GaussianMixture(n_components=nclusters)
    gmm.fit(X)

    op = gmm.predict(X)
    time_speaker = {}
    for i in range(len(op)):
        time_speaker[i + 1] = str(op[i])

    # Segmentation Algorithm

    def getCount(time_speaker, cluster, wstart, wend):
        count = 0
        lastSeenAt = None
        for i in range(wstart, wend + 1):
            if (time_speaker[i] == cluster):
                count += 1
                lastSeenAt = i
        return (count, lastSeenAt)

    def getSuccessor(time_speaker, currentCluster, wstart, params):
        totalDuration = len(time_speaker)
        lookahead = int(params['lookaheadTime'] / chunk_time)
        i = wstart
        successorCount = 0
        while (i <= totalDuration - lookahead):
            j = i + lookahead
            successor = time_speaker[i]
            if (successor == currentCluster):
                return (i, successor)
            successorCount = getCount(time_speaker, successor, i, j)
            if (successorCount[0] > int(params['epsilon'] / chunk_time)):
                return (i, successor)
            i += 1
        i = min(i + lookahead, totalDuration)
        return (i, time_speaker[i])

    def getBreakPoint(time_speaker, cluster, wstart, params):
        totalDuration = len(time_speaker)
        i = wstart
        end = None
        while (time_speaker[i] == cluster):
            i += 1
        breaker = time_speaker[i]
        j = min(i + int(params['lookaheadTime'] / chunk_time), totalDuration)
        breakerCount = getCount(time_speaker, breaker, i, j)
        clusterCount = getCount(time_speaker, cluster, i, j)
        if (breakerCount[0] >= int(params['epsilon'] / chunk_time)):
            end = i
        else:
            i += 1
            successor = getSuccessor(time_speaker, cluster, i, params)
            if (successor[1] == cluster):
                i = successor[0]
                end = getBreakPoint(time_speaker, cluster, i, params)
            else:
                end = successor[0]
        return end

    def segment(time_speaker, params):
        segments = {}
        totalDuration = len(time_speaker)
        w = 1
        recorded = {}
        while (w <= totalDuration):
            cluster = time_speaker[w]
            start = None
            end = None
            if (recorded.get(cluster) == None):
                i = w
                j = min(w + int(params['lookaheadTime'] / chunk_time), totalDuration)
                count = getCount(time_speaker, cluster, i, j)
                if (count[0] > int(params['epsilon'] / chunk_time)):
                    start = i
                    end = getBreakPoint(time_speaker, cluster, start, params)
                    w = end - 1
                    segments[cluster] = (start, end)
                    recorded[cluster] = True
                    continue
            w += 1
        return segments

    params = {
        'lookaheadTime': 7,
        'epsilon': 4
    }
    segments = segment(time_speaker, params)
    segments

    speakers = {}
    rate, data = wavfile.read(wavFile)
    i = 1
    for k in segments:
        start = math.ceil(rate * segments[k][0] * chunk_time)
        end = math.ceil(rate * segments[k][1] * chunk_time)
        speakers[i] = data[start:end + 1]
        i += 1
    speakers
    for i in speakers:
        wavfile.write('Vt/' + str(i) + '.wav', rate, speakers[i])

    len(speakers)

    import nltk
    nltk.download('punkt')

    import speech_recognition as sr
    import os
    from pydub import AudioSegment
    from pydub.silence import split_on_silence
    import pandas as pd

    # speech recognizer
    recognizer = sr.Recognizer()
    directoryPath = "./Vt/"

    # split audio file into chunks and convert audio to text
    def get_audio_paths(path):
        audio = AudioSegment.from_wav(path)
        # split audio sound where silence is 700 miliseconds or more and get chunks
        chunks = split_on_silence(audio, min_silence_len=1000, silence_thresh=audio.dBFS - 16, keep_silence=800)
        folder_name = directoryPath + "/chunks"
        # store chunks in the following directory (create if not exists)
        if not os.path.isdir(folder_name):
            os.mkdir(folder_name)
        final_text = ""
        # for each chunk
        for i, chunk in enumerate(chunks, start=1):

            filename = os.path.join(folder_name, f"chunk{i}.wav")
            chunk.export(filename, format="wav")
            # recognize the audio
            with sr.AudioFile(filename) as source:
                recognizer.adjust_for_ambient_noise(source, duration=0.4)
                audio_listened = recognizer.record(source)
                # convert audio to text
                try:
                    text = recognizer.recognize_google(audio_listened, language='ar')
                    print(text)
                except sr.UnknownValueError as e:
                    print("Error:", str(e))
                else:
                    final_text += text.lower() + "."
        return final_text

    paths = []
    for i, file in enumerate(os.listdir(directoryPath)):
        if file.endswith(".wav"):
            print(os.path.basename(file))
            paths.append(os.path.basename(file))

    text_array = []
    for i, path in enumerate(paths):
        text_array.append(get_audio_paths(directoryPath + path))

    dataframe = pd.DataFrame(text_array, columns=['Text'])
    dataframe.to_csv("text.csv")

    # Text clustering
    # Data in excel sheet with Column "Text"
    statusFile = 'text.csv'
    df = pd.read_csv(statusFile, index_col=0);
    df = df.dropna()
    # speech_obj={}
    speech_obj = []

    count = 1
    for index, row in df.iterrows():
        speech = row.Text.split(".")
        for sentence in speech:
            speech_obj.append({"المتحدث" + str(count): sentence})

    for item in speech_obj:
        print(item)

    ttx = dataframe.iloc[0]['Text']
    tokenized_txt = nltk.word_tokenize(ttx)
    print(tokenized_txt)
    dic = {'الدفاع المدني': 'حريق', 'المرور': 'عطل', 'جنائي': "مخدرات", 'جنائي': 'اختطاف', 'غير محدد': ' '}

    val = dict((v, k) for k, v in dic.items())

    def show_key(x):
        try:
            return val[x]
        except:
            pass

    for i in tokenized_txt:
        if show_key(i) != None:
            speech_obj.append("تصنيف البلاغ  > " + show_key(i))
            print("تصنيف البلاغ  > ", show_key(i))

    return {'list': list1,'topic':topic}

@app.post("/uploade3")
async def create_upload_file(request: Request, file: UploadFile = File(...)):
    with open("1.wav", "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    wavFile = '1.wav'
    AUDIO_FILE = wavFile
    y, sr = librosa.load(AUDIO_FILE)
    Audio(data=y, rate=sr)
    totDuration = librosa.get_duration(filename=AUDIO_FILE)
    # Create chunks/windows for feeding into the model
    division_per_second = 1
    chunk_time = 1.0 / division_per_second
    chunk_size = sr // division_per_second

    remainder_chunks = y.shape[0] % chunk_size
    num_of_chunks = 1
    if (remainder_chunks > 0):
        num_of_chunks = y[:-remainder_chunks].shape[0] / chunk_size
        Y = np.split(y[:-remainder_chunks], num_of_chunks)
    else:
        num_of_chunks = y.shape[0] / chunk_size
        Y = np.split(y, num_of_chunks)

    # Extract feature: Mel-frequency Cepstral Coefficients
    feature_mfcc = np.array([librosa.feature.mfcc(y=chunk, sr=sr, n_mfcc=40) for chunk in Y])
    feature_mfcc.shape
    feature_mfcc_mean = np.mean(feature_mfcc, axis=2)
    # Extract feature: Spectral flatness
    feature_spectral_flatness = np.array([librosa.feature.spectral_flatness(y=y) for chunk in Y])
    feature_spectral_flatness.shape
    feature_spectral_flatness_mean = np.mean(feature_spectral_flatness, axis=2)
    # Extract feature: Spectral flux
    feature_specflux = np.array([librosa.onset.onset_strength(y=y, sr=sr) for chunk in Y])
    feature_specflux.shape
    feature_specflux_mean = np.mean(feature_specflux, axis=1).reshape(-1, 1)
    # Extract feature: Pitch
    feature_pitches = np.array([librosa.piptrack(y=y, sr=sr)[0] for chunk in Y])
    feature_pitches.shape
    feature_pitches_mean = np.mean(feature_pitches, axis=2)
    # Create final feature space for feeding into the model
    X = np.hstack((
        feature_mfcc_mean,
        feature_spectral_flatness_mean,
        feature_specflux_mean,
        feature_pitches_mean
    ))
    X.shape
    # Normalize the input
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    X.shape
    from sklearn.mixture import GaussianMixture
    nclusters = 6
    gmm = GaussianMixture(n_components=nclusters)
    gmm.fit(X)

    op = gmm.predict(X)
    time_speaker = {}
    for i in range(len(op)):
        time_speaker[i + 1] = str(op[i])

    # Segmentation Algorithm

    def getCount(time_speaker, cluster, wstart, wend):
        count = 0
        lastSeenAt = None
        for i in range(wstart, wend + 1):
            if (time_speaker[i] == cluster):
                count += 1
                lastSeenAt = i
        return (count, lastSeenAt)

    def getSuccessor(time_speaker, currentCluster, wstart, params):
        totalDuration = len(time_speaker)
        lookahead = int(params['lookaheadTime'] / chunk_time)
        i = wstart
        successorCount = 0
        while (i <= totalDuration - lookahead):
            j = i + lookahead
            successor = time_speaker[i]
            if (successor == currentCluster):
                return (i, successor)
            successorCount = getCount(time_speaker, successor, i, j)
            if (successorCount[0] > int(params['epsilon'] / chunk_time)):
                return (i, successor)
            i += 1
        i = min(i + lookahead, totalDuration)
        return (i, time_speaker[i])

    def getBreakPoint(time_speaker, cluster, wstart, params):
        totalDuration = len(time_speaker)
        i = wstart
        end = None
        while (time_speaker[i] == cluster):
            i += 1
        breaker = time_speaker[i]
        j = min(i + int(params['lookaheadTime'] / chunk_time), totalDuration)
        breakerCount = getCount(time_speaker, breaker, i, j)
        clusterCount = getCount(time_speaker, cluster, i, j)
        if (breakerCount[0] >= int(params['epsilon'] / chunk_time)):
            end = i
        else:
            i += 1
            successor = getSuccessor(time_speaker, cluster, i, params)
            if (successor[1] == cluster):
                i = successor[0]
                end = getBreakPoint(time_speaker, cluster, i, params)
            else:
                end = successor[0]
        return end

    def segment(time_speaker, params):
        segments = {}
        totalDuration = len(time_speaker)
        w = 1
        recorded = {}
        while (w <= totalDuration):
            cluster = time_speaker[w]
            start = None
            end = None
            if (recorded.get(cluster) == None):
                i = w
                j = min(w + int(params['lookaheadTime'] / chunk_time), totalDuration)
                count = getCount(time_speaker, cluster, i, j)
                if (count[0] > int(params['epsilon'] / chunk_time)):
                    start = i
                    end = getBreakPoint(time_speaker, cluster, start, params)
                    w = end - 1
                    segments[cluster] = (start, end)
                    recorded[cluster] = True
                    continue
            w += 1
        return segments

    params = {
        'lookaheadTime': 7,
        'epsilon': 4
    }
    segments = segment(time_speaker, params)
    segments

    speakers = {}
    rate, data = wavfile.read(wavFile)
    i = 1
    for k in segments:
        start = math.ceil(rate * segments[k][0] * chunk_time)
        end = math.ceil(rate * segments[k][1] * chunk_time)
        speakers[i] = data[start:end + 1]
        i += 1
    speakers
    for i in speakers:
        wavfile.write('Vt/' + str(i) + '.wav', rate, speakers[i])

    len(speakers)

    import nltk
    # nltk.download('punkt')

    import speech_recognition as sr
    import os
    from pydub import AudioSegment
    from pydub.silence import split_on_silence
    import pandas as pd

    # speech recognizer
    recognizer = sr.Recognizer()
    directoryPath = "./Vt/"

    # split audio file into chunks and convert audio to text
    def get_audio_paths(path):
        audio = AudioSegment.from_wav(path)
        # split audio sound where silence is 700 miliseconds or more and get chunks
        chunks = split_on_silence(audio, min_silence_len=1000, silence_thresh=audio.dBFS - 16, keep_silence=800)
        folder_name = directoryPath + "/chunks"
        # store chunks in the following directory (create if not exists)
        if not os.path.isdir(folder_name):
            os.mkdir(folder_name)
        final_text = ""
        # for each chunk
        for i, chunk in enumerate(chunks, start=1):

            filename = os.path.join(folder_name, f"chunk{i}.wav")
            chunk.export(filename, format="wav")
            # recognize the audio
            with sr.AudioFile(filename) as source:
                recognizer.adjust_for_ambient_noise(source, duration=0.4)
                audio_listened = recognizer.record(source)
                # convert audio to text
                try:
                    text = recognizer.recognize_google(audio_listened, language='ar')
                    print(text)
                except sr.UnknownValueError as e:
                    print("Error:", str(e))
                else:
                    final_text += text.lower() + "."
        return final_text

    paths = []
    for i, file in enumerate(os.listdir(directoryPath)):
        if file.endswith(".wav"):
            print(os.path.basename(file))
            paths.append(os.path.basename(file))

    list1 = [] #first defindd
    for i, path in enumerate(paths):
        list1.append(get_audio_paths('1.wav'))

    dataframe = pd.DataFrame(list1, columns=['Text'])
    dataframe.to_csv("sample.csv")

    statusFile = dataframe
    df = dataframe
    df = df.dropna()
    # speech_obj={}
    speech_obj = []

    count = 1
    for index, row in df.iterrows():
        speech = row.Text.split(".")
        for sentence in speech:
            list1.append({"المتحدث" + str(count): sentence})
    #
    # for item in list1:
    #     print(item)
  #end of stt

    ttx = dataframe.iloc[0]['Text']
    tokenized_txt = nltk.word_tokenize(ttx)
    dic = {'الدفاع المدني': 'حريق', 'المرور': 'عطل', 'جنائي': "مخدرات", 'جنائي': 'اختطاف', 'غير محدد': ' '}

    val = dict((v, k) for k, v in dic.items())
    topic = "غير محدد"
    def show_key(x):
        try:
            return val[x]
        except:
            pass

    for i in tokenized_txt:
        if show_key(i) != None:
            speech_obj.append("تصنيف البلاغ  > " + show_key(i))
            topic = show_key(i)
            print("تصنيف البلاغ  > ", show_key(i))
    print('test')
    print(list1)
    return {'list': list1,'topic':topic}